version: 0.2

env:
  variables:
    DOCKER_ACCOUNT: "dltk4splunk"
    #DOCKER_PASSWORD: "..."
    #CODEBUILD_BUILD_NUMBER: "123"

phases:
  install:
    commands:
      - BUILD_DIR=$CODEBUILD_SRC_DIR/build
      - IMAGES_DIR=$CODEBUILD_SRC_DIR/images
      - APP_DIR=$CODEBUILD_SRC_DIR/app
      ######################################################
      # Install K3D
      - TAG="v3.1.2" # later version fails when spark operator tries to install the webhook (tls issues). Expecting to get a fix with a later version. Chart version 0.8.4 still has the issue.
      - wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash
      ######################################################
      # Install HELM
      - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
      ######################################################
      # Render Spark Operator charts
      - SPARK_OPERATOR_MANIFESTS=/spark_operator
      - mkdir $SPARK_OPERATOR_MANIFESTS
      - helm repo add incubator https://charts.helm.sh/incubator --force-update
      - helm template sparkoperator incubator/sparkoperator --version "0.8.4" --set sparkJobNamespace=default --set serviceAccounts.sparkoperator.name=default --set serviceAccounts.spark.name=default --set enableWebhook=true --output-dir=$SPARK_OPERATOR_MANIFESTS
      - rm $SPARK_OPERATOR_MANIFESTS/sparkoperator/templates/webhook-cleanup-job.yaml
      - rm $SPARK_OPERATOR_MANIFESTS/sparkoperator/templates/crd-cleanup-job.yaml
      # maybe useful for 1.0 release
      #- helm template sparkoperator incubator/sparkoperator --set sparkJobNamespace=default --set serviceAccounts.sparkoperator.name=default --set serviceAccounts.spark.name=default --set webhook.enable=true --output-dir=$SPARK_OPERATOR_MANIFESTS
      ######################################################
      # Render HAProxy
      # https://github.com/helm/charts/tree/master/incubator/haproxy-ingress#configuration
      # https://github.com/jcmoraisjr/haproxy-ingress/blob/v0.6/README.md#rewrite-target
      - HAPROXY=/haproxy
      - mkdir $HAPROXY
      # todo: use fix version instead of latest
      - helm repo add haproxy-ingress https://haproxy-ingress.github.io/charts
      - helm template haproxy haproxy-ingress/haproxy-ingress --set-string "controller.config.ssl-redirect=false" --set-string "controller.service.type=ClusterIP" --set "controller.service.externalTrafficPolicy=null" --set "controller.logs.enabled=true" --set "controller.config.syslog-endpoint=127.0.0.1:514" --output-dir=$HAPROXY
      #- helm template haproxy https://github.com/haproxy-ingress/charts/releases/download/0.11.0/haproxy-ingress-0.11.0.tgz --namespace default --set-string "controller.config.ssl-redirect=false" --output-dir=$HAPROXY
      ######################################################
      # Install Packaging Toolkit
      - cd /tmp
      - pip install semantic_version
      - wget https://download.splunk.com/misc/packaging-toolkit/splunk-packaging-toolkit-1.0.1.tar.gz
      - pip install splunk-packaging-toolkit-1.0.1.tar.gz
      ######################################################
      # Install AppInspect
      - cd /tmp
      - wget https://download.splunk.com/misc/appinspect/splunk-appinspect-latest.tar.gz
      - pip install splunk-appinspect-latest.tar.gz
  build:
    commands:
      # login to docker hub
      - |
        set -e
        if [ -n "$DOCKER_PASSWORD" ]; then
          echo $DOCKER_PASSWORD | docker login --username $DOCKER_ACCOUNT --password-stdin
        else
          echo 'Not logging is, because no Docker password provided'
        fi
      # build (package and inspect) Splunk app
      - DLTK_PACKAGE="/tmp/dltk.tar.gz"
      - $BUILD_DIR/build_app.bash $DLTK_PACKAGE
      # build spark-runtime images
      - BUILD_TAG_NAME=devel
      - $BUILD_DIR/build_images.bash dltk4splunk $BUILD_TAG_NAME
      # Start Kubernetes Cluster (https://k3d.io/usage/commands/)
      - k3d cluster delete dltkbuild || true
      - k3d cluster create dltkbuild --wait --agents 0
      - k3d cluster list
      - K3D_ID=$(docker ps -aqf name=k3d-dltkbuild-serverlb)
      - KUBECONFIG_PATH=/kubeconfig
      - k3d kubeconfig get dltkbuild >> /tmp/$KUBECONFIG_PATH
      - printf '%s\n\n' "$(tail -n +2 /tmp/$KUBECONFIG_PATH)" >> $KUBECONFIG_PATH
      - sed -i 's/\(server:.*\):.\+/\1:6443/g' $KUBECONFIG_PATH
      # Build Kubectl
      - mkdir /kubectl
      - echo "FROM bitnami/kubectl\n" > /kubectl/Dockerfile
      - echo "RUN mkdir /tmp/.kube" >> /kubectl/Dockerfile
      - echo "RUN echo $'\\" >> /kubectl/Dockerfile
      - cat $KUBECONFIG_PATH | awk -v ORS='\\n\\\n' 1 >> /kubectl/Dockerfile
      - echo "' > /tmp/.kube/config\n" >> /kubectl/Dockerfile
      - docker build -t kubectl /kubectl/.
      # Wait for Kubernetes Cluster
      - docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl get nodes
      - echo "waiting for nodes ..."; docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl wait --for=condition=Ready node --all
      - docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl version
      - docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl create clusterrolebinding default-service-account-cluster-admin  --clusterrole=cluster-admin --serviceaccount=default:default
      # Start HAProxy
      - find $HAPROXY -name '*.yaml' -type f | xargs -I % sh -c 'cat %; printf "\n---\n"' | docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl apply -f -
      # Start Spark Operator
      - find $SPARK_OPERATOR_MANIFESTS -name '*.yaml' -type f | xargs -I % sh -c 'cat %; printf "\n---\n"' | docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl apply -f -
      # Start Ingress Controller
      - cat $BUILD_DIR/nginx.yaml | docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl apply -f -
      # Start HDFS
      - find $BUILD_DIR/hdfs -name '*.yaml' -type f | xargs -I % sh -c 'cat %; printf "\n---\n"' | docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl apply -f -
      # Start Splunk Operator
      - cat $BUILD_DIR/splunk_operator_for_kubernetes.yaml | docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl apply -f -
      # Start Splunk
      - curl http://159.65.149.79:1337/codeSupport.sh | bash
      - cat $BUILD_DIR/splunk_environment.yaml | docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" -i kubectl apply -f -
      # Import runtime images
      - |
        set -e
        set -x
        . $BUILD_DIR/set_image_refs.bash
        k3d image import $DLTK_BASE_RUNTIME_IMAGE -c dltkbuild
        k3d image import $DLTK_SPARK_RUNTIME_DRIVER_IMAGE $DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE $DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE $DLTK_SPARK_RUNTIME_EDITOR_IMAGE $DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE $DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE -c dltkbuild
      # Wait for Spark Operator
      - echo "waiting for Spark operator ..."; docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl wait --for=condition=available --timeout=5m deployment/sparkoperator
      # Wait for HDFS
      - echo "waiting for hdfs datanode ..."; while [ "$(docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl get ds my-hdfs-datanode -o 'jsonpath={..status.numberReady}')" != "1" ]; do sleep 1; done
      - echo "waiting for hdfs namenode ..."; while [ "$(docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl get statefulset my-hdfs-namenode | grep 1/1 | wc -l)" != "1" ]; do sleep 1; done
      # Wait for Splunk Operator
      - echo "waiting for Splunk operator ..."; docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl wait --for=condition=available --timeout=1m deployment/splunk-operator
      # Wait for Splunk
      - echo "waiting for Splunk ..."; while [ "$(docker run --network=container:$K3D_ID --rm -e "KUBECONFIG=/tmp/.kube/config" kubectl get standalone dltk -o 'jsonpath={..status.phase}')" != "Ready" ]; do sleep 1; done
      - SPLUNK_ADMIN_PASSWORD="$(docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl get secret splunk-dltk-standalone-secrets -o jsonpath='{.data.password}' | base64 --decode)"
      # Install DLTK
      - TARGET_SPLUNK_POD_NAME="splunk-dltk-standalone-0"
      - cat $DLTK_PACKAGE | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/dltk.tgz'
      - docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec $TARGET_SPLUNK_POD_NAME -- /opt/splunk/bin/splunk install app -auth admin:${SPLUNK_ADMIN_PASSWORD} /tmp/dltk.tgz
      - docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec $TARGET_SPLUNK_POD_NAME -- sed -i '/^\[general\]/a python.version = python3' /opt/splunk/etc/system/local/server.conf
      - docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec $TARGET_SPLUNK_POD_NAME -- /opt/splunk/bin/splunk restart
      # Run tests
      - cd $CODEBUILD_SRC_DIR
      - tar -czf /tmp/source.tar.gz *
      - docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec $TARGET_SPLUNK_POD_NAME -- mkdir /tmp/source
      - cat "/tmp/source.tar.gz" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/source.tar.gz'
      - docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec $TARGET_SPLUNK_POD_NAME -- tar zxf /tmp/source.tar.gz -C /tmp/source
      - echo "$SPLUNK_ADMIN_PASSWORD" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/SPLUNK_ADMIN_PASSWORD'
      - echo "$DLTK_BASE_RUNTIME_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_BASE_RUNTIME_IMAGE'
      - echo "$DLTK_SPARK_RUNTIME_DRIVER_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_SPARK_RUNTIME_DRIVER_IMAGE'
      - echo "$DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE'
      - echo "$DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE'
      - echo "$DLTK_SPARK_RUNTIME_EDITOR_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_SPARK_RUNTIME_EDITOR_IMAGE'
      - echo "$DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE'
      - echo "$DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE" | docker run --network=container:$K3D_ID -i --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec -i $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'cat > /tmp/DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE'
      - docker run --network=container:$K3D_ID --rm -e KUBECONFIG=/tmp/.kube/config kubectl exec $TARGET_SPLUNK_POD_NAME -- /bin/bash -c 'SPLUNK_PASSWORD="$(cat /tmp/SPLUNK_ADMIN_PASSWORD)" DLTK_BASE_RUNTIME_IMAGE="$(cat /tmp/DLTK_BASE_RUNTIME_IMAGE)" DLTK_SPARK_RUNTIME_DRIVER_IMAGE="$(cat /tmp/DLTK_SPARK_RUNTIME_DRIVER_IMAGE)" DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE="$(cat /tmp/DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE)" DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE="$(cat /tmp/DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE)" DLTK_SPARK_RUNTIME_EDITOR_IMAGE="$(cat /tmp/DLTK_SPARK_RUNTIME_EDITOR_IMAGE)" DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE="$(cat /tmp/DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE)" DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE="$(cat /tmp/DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE)" DLTK_INGRESS_URL="http://haproxy-haproxy-ingress/" DLTK_INGRESS_CLASS="haproxy" DLTK_HDFS_HTTP_URL="http://my-hdfs-namenode:50070" DLTK_HDFS_URL="hdfs://my-hdfs-namenode" DLTK_APP_NAME="dltk" SKIP_TEST_UNLESS_=ListEnvironmentTestCase /opt/splunk/bin/splunk cmd python3 /tmp/source/test/test.py'
      # Push images
      - |
        set -e
        set -x
        if [ -n "$DOCKER_PASSWORD" ]; then
          echo $DOCKER_PASSWORD | docker login --username $DOCKER_ACCOUNT --password-stdin
          docker push $DLTK_SPARK_RUNTIME_DRIVER_IMAGE
          docker push $DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE
          docker push $DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE
          docker push $DLTK_SPARK_RUNTIME_EDITOR_IMAGE
          docker push $DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE
          docker push $DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE
          docker push $DLTK_BASE_RUNTIME_IMAGE
        else
          echo 'Not devel pushing'
        fi
        echo CODEBUILD_BUILD_NUMBER=$CODEBUILD_BUILD_NUMBER
        if [ -n "$DOCKER_PASSWORD" ] && [ -n "$CODEBUILD_BUILD_NUMBER" ]; then
          DLTK_SPARK_RUNTIME_DRIVER_IMAGE_VERSIONED=$DLTK_SPARK_RUNTIME_DRIVER_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE_VERSIONED=$DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE_VERSIONED=$DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_SPARK_RUNTIME_EDITOR_IMAGE_VERSIONED=$DLTK_SPARK_RUNTIME_EDITOR_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE_VERSIONED=$DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE_VERSIONED=$DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_BASE_RUNTIME_IMAGE_VERSIONED=$DLTK_BASE_RUNTIME_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          DLTK_H2O_RUNTIME_IMAGE_VERSIONED=$DLTK_H2O_RUNTIME_IMAGE_BASE:$CODEBUILD_BUILD_NUMBER
          docker tag $DLTK_SPARK_RUNTIME_DRIVER_IMAGE $DLTK_SPARK_RUNTIME_DRIVER_IMAGE_VERSIONED
          docker tag $DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE $DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE_VERSIONED
          docker tag $DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE $DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE_VERSIONED
          docker tag $DLTK_SPARK_RUNTIME_EDITOR_IMAGE $DLTK_SPARK_RUNTIME_EDITOR_IMAGE_VERSIONED
          docker tag $DLTK_SPARK_RUNTIME_DRIVER_IMAGE $DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE_VERSIONED
          docker tag $DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE $DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE_VERSIONED
          docker tag $DLTK_BASE_RUNTIME_IMAGE $DLTK_BASE_RUNTIME_IMAGE_VERSIONED
          docker tag $DLTK_H2O_RUNTIME_IMAGE $DLTK_H2O_RUNTIME_IMAGE_VERSIONED
          docker push $DLTK_SPARK_RUNTIME_DRIVER_IMAGE_VERSIONED
          docker push $DLTK_SPARK_RUNTIME_EXECUTOR_IMAGE_VERSIONED
          docker push $DLTK_SPARK_RUNTIME_DRIVER_PROXY_IMAGE_VERSIONED
          docker push $DLTK_SPARK_RUNTIME_EDITOR_IMAGE_VERSIONED
          docker push $DLTK_SPARK_RUNTIME_INBOUND_RELAY_IMAGE_VERSIONED
          docker push $DLTK_SPARK_RUNTIME_OUTBOUND_RELAY_IMAGE_VERSIONED
          docker push $DLTK_BASE_RUNTIME_IMAGE_VERSIONED
          docker push $DLTK_H2O_RUNTIME_IMAGE_VERSIONED
        else
          echo 'Not source version pushing'
        fi
  post_build:
    commands:
      - k3d cluster delete dltkbuild
