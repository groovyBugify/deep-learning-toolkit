[default]
runtime = 
description = 
category = 
source_code = 
source_code_version = 0
deployment_code = 
deployment_code_version = 0
# https://docs.splunk.com/Documentation/Splunk/latest/Search/Typesofcommands
# https://conf.splunk.com/files/2017/slides/extending-spl-with-custom-search-commands-and-the-splunk-sdk-for-python.pdf
# streaming: one-by-one, can be pushed to indexers
# stateful: one-by-one, sh-only, no re-ordering
# events: sh-only, may re-order
# reporting: sh-only, for stats/etc
command_type = reporting
default_method = 
max_buffer_size = auto
support_preop = false

# mltk fit: EVENTS
# mltk apply: streaming or stateful




[Binary Neural Network Classifier:fit]
[Binary Neural Network Classifier]
runtime = base
description = Binary neural network classifier build on keras and TensorFlow
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Logistic Regression:fit]
[Logistic Regression]
runtime = base
description = Logistic regression using PyTorch
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Multi Class Neural Network Classifier:fit]

[Multi Class Neural Network Classifier]
runtime = base
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for PyTorch"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Logistic Regression in PyTorch\n",\
    "This notebook contains an example for a simple logistic regression in PyTorch.<br>By default every time you save this notebook the cells are exported into a python module which is then used for executing your custom model invoked by Splunk MLTK Container App. "\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import scipy as sp\n",\
    "import pandas as pd\n",\
    "import torch\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"scipy version: \" + sp.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"PyTorch: \" + torch.__version__)\n",\
    "if torch.cuda.is_available():\n",\
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",\
    "    for i in range(0,torch.cuda.device_count()):\n",\
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",\
    "else:\n",\
    "    print(\"No GPU found\")"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared sample dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>\n",\
    "| fit MLTKContainer mode=stage algo=pytorch_nn epochs=10 species from petal_length petal_width sepal_length sepal_width into app:PyTorch_iris_model_nn"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"PyTorch_iris_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"PyTorch_iris_model_nn\")\n",\
    "#print(param)\n",\
    "print(df.describe)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "def init(df,param):\n",\
    "    print(\"Wahnsinn\")\n",\
    "    print(type(df))\n",\
    "    print(df)\n",\
    "    print(df.describe())\n",\
    "\n",\
    "    print(type(param['feature_variables']))\n",\
    "    print(param['feature_variables'])\n",\
    "    print(type(param['target_variables']))\n",\
    "    print(param['target_variables'])\n",\
    "\n",\
    "    X = df[param['feature_variables']]\n",\
    "    print(type(X))\n",\
    "\n",\
    "    Y = df[param['target_variables']]\n",\
    "    print(type(Y))\n",\
    "\n",\
    "    print(\"Features\")\n",\
    "    print(X.describe())\n",\
    "    \n",\
    "    print(\"Target\")\n",\
    "    print(Y.describe())\n",\
    "    \n",\
    "    input_size = int(X.shape[1])\n",\
    "    num_classes = len(np.unique(Y.to_numpy()))\n",\
    "    learning_rate = 0.001\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\n",\
    "    print(\"FIT build neural network model with input shape \" + str(X.shape))\n",\
    "    print(\"FIT build model with target classes \" + str(num_classes))\n",\
    "    model = {\n",\
    "        \"input_size\": input_size,\n",\
    "        \"num_classes\": num_classes,\n",\
    "        \"learning_rate\": learning_rate,\n",\
    "        \"mapping\": mapping,\n",\
    "        \"num_epochs\": 10000,\n",\
    "        \"batch_size\": 100,\n",\
    "        \"hidden_layers\" : 10,\n",\
    "    }\n",\
    "    device = None\n",\
    "    if torch.cuda.is_available():\n",\
    "        device = torch.device('cuda')\n",\
    "    else:\n",\
    "        device = torch.device('cpu')\n",\
    "    model['device'] = device\n",\
    "    \n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "            if 'hidden_layers' in param['options']['params']:\n",\
    "                model['hidden_layers'] = int(param['options']['params']['hidden_layers'])\n",\
    "\n",\
    "    # Simple neural network model\n",\
    "    model['model'] = torch.nn.Sequential(\n",\
    "        torch.nn.Linear(model['input_size'], model['hidden_layers']),\n",\
    "        torch.nn.ReLU(),\n",\
    "        torch.nn.Linear(model['hidden_layers'], model['num_classes']),\n",\
    "    ).to(model['device'])\n",\
    "\n",\
    "    # Define loss and optimizer\n",\
    "    model['criterion'] = torch.nn.CrossEntropyLoss()  \n",\
    "    model['optimizer'] = torch.optim.SGD(model['model'].parameters(), lr=learning_rate)      \n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y)) }\n",\
    "    Y = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy().reshape(-1)\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    print(model['num_epochs'])\n",\
    "\n",\
    "    inputs = torch.from_numpy(X).to(model['device'])\n",\
    "    targets = torch.from_numpy(Y).to(model['device'])\n",\
    "\n",\
    "    for epoch in range(model['num_epochs']):\n",\
    "        outputs = model['model'](inputs)\n",\
    "        loss = model['criterion'](outputs, targets)\n",\
    "        model['optimizer'].zero_grad()\n",\
    "        loss.backward()\n",\
    "        model['optimizer'].step()\n",\
    "        if (epoch+1) % (model['num_epochs']/100) == 0:\n",\
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, model['num_epochs'], loss.item()))                \n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model['num_epochs']\n",\
    "    returns['model_batch_size'] = model['batch_size']\n",\
    "    returns['model_loss_acc'] = loss.item()\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    classes = {v: k for k, v in model['mapping'].items()}\n",\
    "    with torch.no_grad():\n",\
    "        input = torch.from_numpy(X).to(model['device'])\n",\
    "        output = model['model'](input)\n",\
    "        y_hat = output.data\n",\
    "        _, predicted = torch.max(output.data, 1)\n",\
    "        predicted = predicted.cpu()\n",\
    "        prediction = [classes[key] for key in predicted.numpy()]\n",\
    "    return pd.concat([df, pd.DataFrame(prediction)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "y_hat = apply(model,df,param)\n",\
    "y_hat"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",\
    "    if model is not None:\n",\
    "        if 'model' in model:\n",\
    "            returns[\"summary\"] = str(model)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 16
deployment_code = #!/usr/bin/env python\
# coding: utf-8\
\
\
    \
# In[ ]:\
\
\
# this definition exposes all python module imports that should be available in all subsequent commands\
import json\
import datetime\
import numpy as np\
import scipy as sp\
import pandas as pd\
import torch\
# global constants\
MODEL_DIRECTORY = "/srv/app/model/data/"\
\
\
\
\
\
\
\
    \
# In[ ]:\
\
\
# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\
def stage(name):\
    with open("data/"+name+".csv", 'r') as f:\
        df = pd.read_csv(f)\
    with open("data/"+name+".json", 'r') as f:\
        param = json.load(f)\
    return df, param\
\
\
\
\
\
\
\
    \
# In[ ]:\
\
\
def init(df,param):\
    print("Wahnsinn")\
    print(type(df))\
    print(df)\
    print(df.describe())\
\
    print(type(param['feature_variables']))\
    print(param['feature_variables'])\
    print(type(param['target_variables']))\
    print(param['target_variables'])\
\
    X = df[param['feature_variables']]\
    print(type(X))\
\
    Y = df[param['target_variables']]\
    print(type(Y))\
\
    print("Features")\
    print(X.describe())\
    \
    print("Target")\
    print(Y.describe())\
    \
    input_size = int(X.shape[1])\
    num_classes = len(np.unique(Y.to_numpy()))\
    learning_rate = 0.001\
    mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\
    print("FIT build neural network model with input shape " + str(X.shape))\
    print("FIT build model with target classes " + str(num_classes))\
    model = {\
        "input_size": input_size,\
        "num_classes": num_classes,\
        "learning_rate": learning_rate,\
        "mapping": mapping,\
        "num_epochs": 10000,\
        "batch_size": 100,\
        "hidden_layers" : 10,\
    }\
    device = None\
    if torch.cuda.is_available():\
        device = torch.device('cuda')\
    else:\
        device = torch.device('cpu')\
    model['device'] = device\
    \
    if 'options' in param:\
        if 'params' in param['options']:\
            if 'epochs' in param['options']['params']:\
                model['num_epochs'] = int(param['options']['params']['epochs'])\
            if 'batch_size' in param['options']['params']:\
                model['batch_size'] = int(param['options']['params']['batch_size'])\
            if 'hidden_layers' in param['options']['params']:\
                model['hidden_layers'] = int(param['options']['params']['hidden_layers'])\
\
    # Simple neural network model\
    model['model'] = torch.nn.Sequential(\
        torch.nn.Linear(model['input_size'], model['hidden_layers']),\
        torch.nn.ReLU(),\
        torch.nn.Linear(model['hidden_layers'], model['num_classes']),\
    ).to(model['device'])\
\
    # Define loss and optimizer\
    model['criterion'] = torch.nn.CrossEntropyLoss()  \
    model['optimizer'] = torch.optim.SGD(model['model'].parameters(), lr=learning_rate)      \
    return model\
\
\
\
\
\
\
\
    \
# In[ ]:\
\
\
def fit(model,df,param):\
    returns = {}\
    X = df[param['feature_variables']].astype('float32').to_numpy()\
    Y = df[param['target_variables']].to_numpy().reshape(-1)\
    mapping = { key: value for value,key in enumerate(np.unique(Y)) }\
    Y = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy().reshape(-1)\
    if 'options' in param:\
        if 'params' in param['options']:\
            if 'epochs' in param['options']['params']:\
                model['num_epochs'] = int(param['options']['params']['epochs'])\
            if 'batch_size' in param['options']['params']:\
                model['batch_size'] = int(param['options']['params']['batch_size'])\
    print(model['num_epochs'])\
\
    inputs = torch.from_numpy(X).to(model['device'])\
    targets = torch.from_numpy(Y).to(model['device'])\
\
    for epoch in range(model['num_epochs']):\
        outputs = model['model'](inputs)\
        loss = model['criterion'](outputs, targets)\
        model['optimizer'].zero_grad()\
        loss.backward()\
        model['optimizer'].step()\
        if (epoch+1) % (model['num_epochs']/100) == 0:\
            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, model['num_epochs'], loss.item()))                \
    # memorize parameters\
    returns['model_epochs'] = model['num_epochs']\
    returns['model_batch_size'] = model['batch_size']\
    returns['model_loss_acc'] = loss.item()\
    return returns\
\
\
\
\
\
\
\
    \
# In[ ]:\
\
\
def apply(model,df,param):\
    X = df[param['feature_variables']].astype('float32').to_numpy()\
    classes = {v: k for k, v in model['mapping'].items()}\
    with torch.no_grad():\
        input = torch.from_numpy(X).to(model['device'])\
        output = model['model'](input)\
        y_hat = output.data\
        _, predicted = torch.max(output.data, 1)\
        predicted = predicted.cpu()\
        prediction = [classes[key] for key in predicted.numpy()]\
    return pd.concat([df, pd.DataFrame(prediction)], axis=1)\
\
\
\
\
\
\
\
    \
# In[ ]:\
\
\
# save model to name in expected convention "<algo_name>_<model_name>.h5"\
def save(model,name):\
    torch.save(model, MODEL_DIRECTORY + name + ".pt")\
    return model\
\
\
\
\
\
    \
# In[ ]:\
\
\
# load model from name in expected convention "<algo_name>_<model_name>.h5"\
def load(name):\
    model = torch.load(MODEL_DIRECTORY + name + ".pt")\
    return model\
\
\
\
\
\
    \
# In[ ]:\
\
\
# return model summary\
def summary(model=None):\
    returns = {"version": {"pytorch": torch.__version__} }\
    if model is not None:\
        if 'model' in model:\
            returns["summary"] = str(model)\
    return returns\
\
\
\
\
\

deployment_code_version = 16

[Multi Class Neural Network Classifier for DGA:fit]
[Multi Class Neural Network Classifier for DGA]
runtime = base
description = Multi class neural network classifier using PyTorch for DGA Detection
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Gradient Boosting Classifier for DGA:fit]
[Gradient Boosting Classifier for DGA]
runtime = spark
description = Gradient boosting model with Spark MLLib applied to the DGA dataset
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Linear Regressor:fit]
[Linear Regressor]
runtime = base
description = Linear regression using the TensorFlow™ estimator class
category = Regressor
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Deep Neural Network Regressor:fit]
[Deep Neural Network Regressor]
runtime = base
description = Regression using the TensorFlow™ Deep Neural Network (DNN) estimator class
category = Regressor
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Forecasting Convolutional Neural Network:fit]
[Forecasting Convolutional Neural Network]
runtime = base
description = Forecasting time series using TensorFlow (CNN)
category = Regressor
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Long Short Term Memory (LSTM) Neural Network:fit]
[Long Short Term Memory (LSTM) Neural Network]
runtime = base
description = Forecasting time series using TensorFlow (LSTM)
category = Forecasting
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Forecasting with Prophet:fit]
[Forecasting with Prophet]
runtime = base
description = Forecasting time series using the Prophet library
category = Forecasting
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Autoencoder:fit]
[Autoencoder]
runtime = base
description = Basic auto encoder using TensorFlow™ 
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Distributed KMeans with Dask:fit]
[Distributed KMeans with Dask]
runtime = base
description = Distribute algorithm execution with DASK for KMeans
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Clustering with UMAP and DBSCAN:fit]
[Clustering with UMAP and DBSCAN]
runtime = base
description = Clustering with UMAP and DBSCAN
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Rapids UMAP:fit]
[Rapids UMAP]
runtime = base
description = Rapids UMAP (GPU accelerated)
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Named Entity Recognition and Extraction:fit]
[Named Entity Recognition and Extraction]
runtime = base
description = Named Entity Recognition using spaCy for NLP tasks
category = NLP
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Named Entity Recognition and Extraction Ginza (Japanese):fit]
[Named Entity Recognition and Extraction Ginza (Japanese)]
runtime = base
description = Named Entity Recognition using spaCy Ginza (Japanese)
category = NLP
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Graph Centrality Algorithms:fit]
[Graph Centrality Algorithms] 
runtime = base
description = Graph centrality algorithms using NetworkX
category = Graphs
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Graph Community Detection:fit]
[Graph Community Detection]
runtime = base
description = Graph community detection with Rapids (GPU accelerated)
category = Graphs
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[FP Growth:fit]
[FP Growth]
runtime = spark
description = Frequent itemset mining with Spark FP Growth
category = Data Mining
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Collaborative Filtering:fit]
[Collaborative Filtering]
runtime = spark
description = Recommender system with Spark Collaborative Filtering
category = Data Mining
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Correlation Matrix:fit]
[Correlation Matrix]
runtime = spark
description = Correlation Matrix and Pair Plot
category = Basic
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Spark Pi:fit]
[Spark Pi]
runtime = spark
description = Spark Pi Hello World example
category = Basic
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}

[Distributed Random Forest Regressor:fit]
[Distributed Random Forest Regressor:fit_only]
[Distributed Random Forest Regressor:fit_and_score]
[Distributed Random Forest Regressor]
runtime = spark
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "import io\n",\
    "import json\n",\
    "import numpy as np\n",\
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",\
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",\
    "from pyspark.mllib.util import MLUtils\n",\
    "from pyspark.mllib.regression import LabeledPoint\n",\
    "from pyspark.mllib.linalg import Vectors\n",\
    "from pyspark.mllib.evaluation import RegressionMetrics\n",\
    "\n",\
    "def fit_only(sc, events):\n",\
    "    # Step 1: Prepare data as labeled points\n",\
    "    training_data = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    \n",\
    "    # Step 2: Train model using Random Forest Regressor\n",\
    "    model = RandomForest.trainRegressor(\n",\
    "        training_data,\n",\
    "        categoricalFeaturesInfo={},\n",\
    "        numTrees=10, \n",\
    "        featureSubsetStrategy=\"auto\",\n",\
    "        impurity=\"variance\",\n",\
    "        maxDepth=4,\n",\
    "        maxBins=32\n",\
    "    )\n",\
    "    return [{\"status\": \"success\"}]\n",\
    "\n",\
    "\n",\
    "\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    \n",\
    "    # Step 1: Prepare data as labeled points\n",\
    "    training_data = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    \n",\
    "    # Step 2: Train model using Random Forest Regressor\n",\
    "    model = RandomForest.trainRegressor(\n",\
    "        training_data,\n",\
    "        categoricalFeaturesInfo={},                        \n",\
    "        numTrees=10, \n",\
    "        featureSubsetStrategy=\"auto\",\n",\
    "        impurity=\"variance\",\n",\
    "        maxDepth=4,\n",\
    "        maxBins=32\n",\
    "    )\n",\
    "\n",\
    "    # Step 3: Compute predictions\n",\
    "    predictions = model.predict(events.map(lambda row: [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "\n",\
    "    # Step 4: Join predictions and return results\n",\
    "    results = events.zip(predictions) \n",\
    "    def format_results(t):\n",\
    "        e,p=t\n",\
    "        e[\"predicted_fare_amount\"]=round(p,1)\n",\
    "        return e\n",\
    "    return results.map(format_results)\n",\
    "\n",\
    "\n",\
    "\n",\
    "def fit_and_score(sc, events):\n",\
    "    rdd = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    model = RandomForest.trainRegressor(rdd, categoricalFeaturesInfo={},\n",\
    "                                numTrees=10, featureSubsetStrategy=\"auto\",\n",\
    "                                impurity=\"variance\", maxDepth=4, maxBins=32)    \n",\
    "\n",\
    "    predictions = model.predict(events.map(lambda row: [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "\n",\
    "    # Join prediction with input events\n",\
    "    results = events.zip(predictions) \n",\
    "    def format_results(t):\n",\
    "        e,p=t\n",\
    "        e[\"predicted_fare_amount\"]=p\n",\
    "        return e\n",\
    "    results = results.map(format_results)\n",\
    "    \n",\
    "\n",\
    "    # Calculate Scoring Metrics\n",\
    "    values_and_predictions = results.map(lambda p: (float(p[\"prediction\"]), float(p[\"fare_amount\"])))\n",\
    "\n",\
    "    metrics = RegressionMetrics(values_and_predictions)\n",\
    "    return [{\"MSE\": metrics.meanSquaredError, \n",\
    "             \"RMSE\": metrics.rootMeanSquaredError, \n",\
    "             \"R2\": metrics.r2, \n",\
    "             \"MAE\": metrics.meanAbsoluteError, \n",\
    "             \"Explained variance\": metrics.explainedVariance, \n",\
    "            }]"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}\

source_code_version = 35
input_hdfs_data_path = direct
executor_instance_count = 4
executor_cores = 2

