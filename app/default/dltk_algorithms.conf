[default]
runtime = 
description = 
category = 
source_code = 
source_code_version = 0
deployment_code = 
deployment_code_version = 0
# https://docs.splunk.com/Documentation/Splunk/latest/Search/Typesofcommands
# https://conf.splunk.com/files/2017/slides/extending-spl-with-custom-search-commands-and-the-splunk-sdk-for-python.pdf
# streaming: one-by-one, can be pushed to indexers
# stateful: one-by-one, sh-only, no re-ordering
# events: sh-only, may re-order
# reporting: sh-only, for stats/etc
command_type = reporting
default_method = 
max_buffer_size = auto
support_preop = false

# mltk fit: EVENTS
# mltk apply: streaming or stateful

[Binary Neural Network Classifier:summary]
[Binary Neural Network Classifier:apply]
[Binary Neural Network Classifier:fit]
[Binary Neural Network Classifier]
runtime = base
description = Binary neural network classifier build on keras and TensorFlow
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for TensorFlow 2.0"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Neural Network for Binary Classification\n",\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk. As an example we use a custom binary neural network classifier built on keras and tensorflow."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_import\n",\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import tensorflow as tf\n",\
    "from tensorflow import keras\n",\
    "\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 2,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.16.2\n",\
      "pandas version: 0.24.2\n",\
      "TensorFlow version: 2.0.0-alpha0\n",\
      "Keras version: 2.2.4-tf\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"TensorFlow version: \" + tf.__version__)\n",\
    "print(\"Keras version: \" + keras.__version__)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv <br>| fit MLTKContainer response from * algo=binary_nn_classifier epochs=10 mode=stage into MyModel"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"my_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage\n",\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"default\")\n",\
    "print(df[0:1])\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_init\n",\
    "# initialize the model\n",\
    "# params: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    print(\"FIT build model with input shape \" + str(X.shape))\n",\
    "    input_shape = int(X.shape[1])\n",\
    "    model_structure = '2-2'\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'structure' in param['options']['params']:\n",\
    "                model_structure = str(param['options']['params']['structure']).lstrip(\"\\\"\").rstrip(\"\\\"\")\n",\
    "    hidden_factors = np.floor(np.array(model_structure.split(\"-\"), dtype=\"float\") * X.shape[1])\n",\
    "    model = keras.Sequential()\n",\
    "    model.add(keras.layers.Dense(input_shape, input_dim=input_shape, activation=tf.nn.relu))\n",\
    "    for hidden in hidden_factors:\n",\
    "        model.add(keras.layers.Dense(int(hidden), activation=tf.nn.relu))\n",\
    "    model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",\
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model\n",\
    "model = init(df,param)\n",\
    "print(model.summary())"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_fit\n",\
    "# returns a fit info json object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']]\n",\
    "    Y = df[param['target_variables']]\n",\
    "    model_epochs = 100\n",\
    "    model_batch_size = None\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model_epochs = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",\
    "    # connect model training to tensorboard\n",\
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",\
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",\
    "    # run the training\n",\
    "    returns['fit_history'] = model.fit(x=X,\n",\
    "                                       y=Y, \n",\
    "                                       verbose=2, \n",\
    "                                       epochs=model_epochs, \n",\
    "                                       batch_size=model_batch_size, \n",\
    "                                       #validation_data=(X, Y),\n",\
    "                                       callbacks=[tensorboard_callback])\n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model_epochs\n",\
    "    returns['model_batch_size'] = model_batch_size\n",\
    "    returns['model_loss_acc'] = model.evaluate(x = X, y = Y)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_apply\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    y_hat = model.predict(x = X, verbose=1)\n",\
    "    return pd.concat([df,pd.DataFrame(y_hat)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model_apply\n",\
    "y_hat = apply(model,df,param)\n",\
    "print(y_hat)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    # save keras model to hdf5 file\n",\
    "    # https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models\n",\
    "    model.save(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",\
    "    if model is not None:\n",\
    "        # Save keras model summary to string:\n",\
    "        s = []\n",\
    "        model.summary(print_fn=lambda x: s.append(x+'\\n'))\n",\
    "        returns[\"summary\"] = ''.join(s)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 2

[Logistic Regression:summary]
[Logistic Regression:apply]
[Logistic Regression:fit]
[Logistic Regression]
runtime = base
description = Logistic Regression in PyTorch
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for PyTorch"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Logistic Regression in PyTorch\n",\
    "This notebook contains an example for a simple logistic regression in PyTorch.<br>By default every time you save this notebook the cells are exported into a python module which is then used for executing your custom model invoked by Splunk MLTK Container App. "\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import scipy as sp\n",\
    "import pandas as pd\n",\
    "import torch\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"scipy version: \" + sp.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"PyTorch: \" + torch.__version__)\n",\
    "if torch.cuda.is_available():\n",\
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",\
    "    for i in range(0,torch.cuda.device_count()):\n",\
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",\
    "else:\n",\
    "    print(\"No GPU found\")"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared sample dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>\n",\
    "| fit MLTKContainer algo=pytorch_logistic_regression epochs=1000 mode=stage species from petal_length petal_width sepal_length sepal_width into app:PyTorch_iris_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"PyTorch_iris_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"PyTorch_iris_model\")\n",\
    "#print(param)\n",\
    "print(df.describe)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    Y = df[param['target_variables']]\n",\
    "    input_size = int(X.shape[1])\n",\
    "    num_classes = len(np.unique(Y.to_numpy()))\n",\
    "    learning_rate = 0.001\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\n",\
    "    print(\"FIT build logistic regression model with input shape \" + str(X.shape))\n",\
    "    print(\"FIT build model with target classes \" + str(num_classes))\n",\
    "    model = {\n",\
    "        \"input_size\": input_size,\n",\
    "        \"num_classes\": num_classes,\n",\
    "        \"learning_rate\": learning_rate,\n",\
    "        \"mapping\": mapping,\n",\
    "        \"num_epochs\": 10000,\n",\
    "        \"batch_size\": 100,\n",\
    "    }\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    # Simple logistic regression model\n",\
    "    model['model'] = torch.nn.Linear(input_size, num_classes)\n",\
    "    # Define loss and optimizer\n",\
    "    model['criterion'] = torch.nn.CrossEntropyLoss()  \n",\
    "    model['optimizer'] = torch.optim.SGD(model['model'].parameters(), lr=learning_rate)      \n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y)) }\n",\
    "    Y = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy().reshape(-1)\n",\
    "    #Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    #Y = pd.get_dummies(Y).astype('float32').to_numpy()\n",\
    "    #Ymap = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy()\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    print(model['num_epochs'])\n",\
    "    for epoch in range(model['num_epochs']):\n",\
    "        inputs = torch.from_numpy(X)\n",\
    "        targets = torch.from_numpy(Y)\n",\
    "        outputs = model['model'](inputs)\n",\
    "        loss = model['criterion'](outputs, targets)\n",\
    "        model['optimizer'].zero_grad()\n",\
    "        loss.backward()\n",\
    "        model['optimizer'].step()\n",\
    "        if (epoch+1) % (model['num_epochs']/10) == 0:\n",\
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, model['num_epochs'], loss.item()))                \n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model['num_epochs']\n",\
    "    returns['model_batch_size'] = model['batch_size']\n",\
    "    returns['model_loss_acc'] = loss.item()\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    classes = {v: k for k, v in model['mapping'].items()}\n",\
    "    with torch.no_grad():\n",\
    "        input = torch.from_numpy(X)\n",\
    "        output = model['model'](input)\n",\
    "        y_hat = output.data\n",\
    "        _, predicted = torch.max(output.data, 1)\n",\
    "        prediction = [classes[key] for key in predicted.numpy()]\n",\
    "    return pd.concat([df, pd.DataFrame(prediction)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "y_hat = apply(model,df,param)\n",\
    "y_hat"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",\
    "    if model is not None:\n",\
    "        if 'model' in model:\n",\
    "            returns[\"summary\"] = str(model)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 4


[Multi Class Neural Network Classifier:summary]
[Multi Class Neural Network Classifier:apply]
[Multi Class Neural Network Classifier:fit]
[Multi Class Neural Network Classifier]
runtime = base
description = Simple multi class neural network classifier using PyTorch with GPU
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for PyTorch"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Multi class neural network in PyTorch\n",\
    "This notebook contains an example for a simple multi class neural network in PyTorch.<br>By default every time you save this notebook the cells are exported into a python module which is then used for executing your custom model invoked by Splunk MLTK Container App. "\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import scipy as sp\n",\
    "import pandas as pd\n",\
    "import torch\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"scipy version: \" + sp.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"PyTorch: \" + torch.__version__)\n",\
    "if torch.cuda.is_available():\n",\
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",\
    "    for i in range(0,torch.cuda.device_count()):\n",\
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",\
    "else:\n",\
    "    print(\"No GPU found\")"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared sample dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>\n",\
    "| fit MLTKContainer mode=stage algo=pytorch_nn epochs=10 species from petal_length petal_width sepal_length sepal_width into app:PyTorch_iris_model_nn"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"PyTorch_iris_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"PyTorch_iris_model_nn\")\n",\
    "print(df.describe)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    Y = df[param['target_variables']]\n",\
    "    input_size = int(X.shape[1])\n",\
    "    num_classes = len(np.unique(Y.to_numpy()))\n",\
    "    learning_rate = 0.001\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\n",\
    "    print(\"FIT build neural network model with input shape \" + str(X.shape))\n",\
    "    print(\"FIT build model with target classes \" + str(num_classes))\n",\
    "    model = {\n",\
    "        \"input_size\": input_size,\n",\
    "        \"num_classes\": num_classes,\n",\
    "        \"learning_rate\": learning_rate,\n",\
    "        \"mapping\": mapping,\n",\
    "        \"num_epochs\": 10000,\n",\
    "        \"batch_size\": 100,\n",\
    "        \"hidden_layers\" : 10,\n",\
    "    }\n",\
    "    device = None\n",\
    "    if torch.cuda.is_available():\n",\
    "        device = torch.device('cuda')\n",\
    "    else:\n",\
    "        device = torch.device('cpu')\n",\
    "    model['device'] = device\n",\
    "    \n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "            if 'hidden_layers' in param['options']['params']:\n",\
    "                model['hidden_layers'] = int(param['options']['params']['hidden_layers'])\n",\
    "\n",\
    "    # Simple neural network model\n",\
    "    model['model'] = torch.nn.Sequential(\n",\
    "        torch.nn.Linear(model['input_size'], model['hidden_layers']),\n",\
    "        torch.nn.ReLU(),\n",\
    "        torch.nn.Linear(model['hidden_layers'], model['num_classes']),\n",\
    "    ).to(model['device'])\n",\
    "\n",\
    "    # Define loss and optimizer\n",\
    "    model['criterion'] = torch.nn.CrossEntropyLoss()  \n",\
    "    model['optimizer'] = torch.optim.SGD(model['model'].parameters(), lr=learning_rate)      \n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y)) }\n",\
    "    Y = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy().reshape(-1)\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    print(model['num_epochs'])\n",\
    "\n",\
    "    inputs = torch.from_numpy(X).to(model['device'])\n",\
    "    targets = torch.from_numpy(Y).to(model['device'])\n",\
    "\n",\
    "    for epoch in range(model['num_epochs']):\n",\
    "        outputs = model['model'](inputs)\n",\
    "        loss = model['criterion'](outputs, targets)\n",\
    "        model['optimizer'].zero_grad()\n",\
    "        loss.backward()\n",\
    "        model['optimizer'].step()\n",\
    "        if (epoch+1) % (model['num_epochs']/100) == 0:\n",\
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, model['num_epochs'], loss.item()))                \n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model['num_epochs']\n",\
    "    returns['model_batch_size'] = model['batch_size']\n",\
    "    returns['model_loss_acc'] = loss.item()\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    classes = {v: k for k, v in model['mapping'].items()}\n",\
    "    with torch.no_grad():\n",\
    "        input = torch.from_numpy(X).to(model['device'])\n",\
    "        output = model['model'](input)\n",\
    "        y_hat = output.data\n",\
    "        _, predicted = torch.max(output.data, 1)\n",\
    "        predicted = predicted.cpu()\n",\
    "        prediction = [classes[key] for key in predicted.numpy()]\n",\
    "    return pd.concat([df, pd.DataFrame(prediction)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "y_hat = apply(model,df,param)\n",\
    "y_hat"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",\
    "    if model is not None:\n",\
    "        if 'model' in model:\n",\
    "            returns[\"summary\"] = str(model)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 20

[Autoencoder:summary]
[Autoencoder:apply]
[Autoencoder:fit]
[Autoencoder]
runtime = base
description = Basic auto encoder using TensorFlow™ 
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for TensorFlow 2.0"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Autoencoder Example\n",\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk. As an example we use a custom autoencoder built on keras and tensorflow."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_import\n",\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import tensorflow as tf\n",\
    "from tensorflow import keras\n",\
    "\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"TensorFlow version: \" + tf.__version__)\n",\
    "print(\"Keras version: \" + keras.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>| fit MLTKContainer algo=autoencoder epochs=100 batch_size=4 components=2 petal_length petal_width sepal_length sepal_width into app:iris_autoencoder"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"my_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage\n",\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"iris_autoencoder\")\n",\
    "print(df[0:1])\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_init\n",\
    "# initialize the model\n",\
    "# params: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    print(\"FIT build model with input shape \" + str(X.shape))\n",\
    "    components = 3\n",\
    "    activation_fn = 'relu'\n",\
    "    # learning_rate = 0.001\n",\
    "    # epsilon=0.00001 # default 1e-07\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'components' in param['options']['params']:\n",\
    "                components = int(param['options']['params']['components'])\n",\
    "            if 'activation_func' in param['options']['params']:\n",\
    "                activation_fn = param['options']['params']['activation_func']\n",\
    "    input_shape = int(X.shape[1])\n",\
    "    encoder_layer = keras.layers.Dense(components, input_dim=input_shape, activation=activation_fn, kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None), bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))\n",\
    "    decoder_layer = keras.layers.Dense(input_shape, activation=activation_fn, kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None), bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))\n",\
    "    model = keras.Sequential()\n",\
    "    model.add(encoder_layer)\n",\
    "    model.add(decoder_layer)\n",\
    "    #opt = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",\
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model\n",\
    "model = init(df,param)\n",\
    "print(model.summary())"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_fit\n",\
    "# returns a fit info json object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']]\n",\
    "    model_epochs = 100\n",\
    "    model_batch_size = 32\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model_epochs = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",\
    "    # connect model training to tensorboard\n",\
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",\
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",\
    "    # run the training\n",\
    "    returns['fit_history'] = model.fit(x=X,\n",\
    "                                       y=X, \n",\
    "                                       verbose=2, \n",\
    "                                       epochs=model_epochs, \n",\
    "                                       batch_size=model_batch_size, \n",\
    "                                       #validation_data=(X, Y),\n",\
    "                                       callbacks=[tensorboard_callback])\n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model_epochs\n",\
    "    returns['model_batch_size'] = model_batch_size\n",\
    "    returns['model_loss_acc'] = model.evaluate(x = X, y = X)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_apply\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    reconstruction = model.predict(x = X)\n",\
    "    intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.layers[0].output)\n",\
    "    hidden = intermediate_layer_model.predict(x = X)\n",\
    "    y_hat = pd.concat([pd.DataFrame(reconstruction).add_prefix(\"reconstruction_\"), pd.DataFrame(hidden).add_prefix(\"hidden_\")], axis=1)\n",\
    "    return pd.concat([df,y_hat], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model_apply\n",\
    "y_hat = apply(model,df,param)\n",\
    "print(y_hat)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    # save keras model to hdf5 file\n",\
    "    # https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models\n",\
    "    model.save(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",\
    "    if model is not None:\n",\
    "        # Save keras model summary to string:\n",\
    "        s = []\n",\
    "        model.summary(print_fn=lambda x: s.append(x+'\\n'))\n",\
    "        returns[\"summary\"] = ''.join(s)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 2

[Distributed KMeans with Dask:fit]
[Distributed KMeans with Dask]
runtime = base
description = Distribute algorithm execution with DASK for KMeans
category = Clustering
source_code_version = 4
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Dask Distributed KMeans"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import dask.dataframe as dd\n",\
    "from dask.distributed import Client\n",\
    "import dask_ml.cluster\n",\
    "\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "import dask_ml\n",\
    "print(\"dask_ml version: \" + dask_ml.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br/>\n",\
    "| fit MLTKContainer algo=dask_kmeans k=3 petal_length petal_width sepal_length sepal_width into app:iris_dask_kmeans"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"iris_dask_kmeans\")\n",\
    "print(df)\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    \n",\
    "    #client = Client(\"tcp://127.0.0.1:\")\n",\
    "    client = Client(processes=False)\n",\
    "    \n",\
    "    model['dask_client'] = client\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    ddf = dd.from_pandas(df, npartitions=4)  \n",\
    "    # features dataframe\n",\
    "    features = ddf[param['feature_variables']]\n",\
    "    #features.persist()\n",\
    "    k = int(param['options']['params']['k'])\n",\
    "    model['dask_kmeans'] = dask_ml.cluster.KMeans(n_clusters=k, init_max_iter=2, oversampling_factor=10)\n",\
    "    model['dask_kmeans'].fit(features)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "%time print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    #ddf = dd.from_pandas(df[param['feature_variables']], npartitions=4)\n",\
    "    prediction = model['dask_kmeans'].labels_\n",\
    "    #y_hat = prediction.to_dask_dataframe()\n",\
    "    result = pd.DataFrame(prediction.compute())\n",\
    "    model['dask_client'].close()\n",\
    "    return pd.concat([df,result], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "%time print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    # TODO\n",\
    "    # model['dask_kmeans'].save_model(MODEL_DIRECTORY + name + '.json')\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "save(model,'dask_kmeans')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    # TODO\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "loaded_model = load('dask_kmeans')\n",\
    "loaded_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"model\": model}\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "summary(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

[Clustering with UMAP and DBSCAN:summary]
[Clustering with UMAP and DBSCAN:apply]
[Clustering with UMAP and DBSCAN:fit]
[Clustering with UMAP and DBSCAN]
runtime = base
description = Clustering with UMAP and DBSCAN
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - UMAP"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example for UMAP that seamlessly interfaces with the Deep Learning Toolkit for Splunk.<br>\n",\
    "<a href=\"https://umap-learn.readthedocs.io/en/latest/api.html\">https://umap-learn.readthedocs.io/en/latest/api.html</a>"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import umap\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 2,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.18.1\n",\
      "pandas version: 1.0.1\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv <br>\n",\
    "| fit MLTKContainer mode=stage algo=umap n_components=3 BMI age blood_pressure diabetes_pedigree glucose_concentration number_pregnant serum_insulin skin_thickness into app:diabetes_umap as umap"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 11,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 12,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "       number_pregnant  glucose_concentration  blood_pressure  skin_thickness  \\\n",\
      "count       768.000000             768.000000      768.000000      768.000000   \n",\
      "mean          3.845052             120.894531       69.105469       20.536458   \n",\
      "std           3.369578              31.972618       19.355807       15.952218   \n",\
      "min           0.000000               0.000000        0.000000        0.000000   \n",\
      "25%           1.000000              99.000000       62.000000        0.000000   \n",\
      "50%           3.000000             117.000000       72.000000       23.000000   \n",\
      "75%           6.000000             140.250000       80.000000       32.000000   \n",\
      "max          17.000000             199.000000      122.000000       99.000000   \n",\
      "\n",\
      "       serum_insulin         BMI  diabetes_pedigree         age    response  \n",\
      "count     768.000000  768.000000         768.000000  768.000000  768.000000  \n",\
      "mean       79.799479   31.992578           0.471876   33.240885    0.348958  \n",\
      "std       115.244002    7.884160           0.331329   11.760232    0.476951  \n",\
      "min         0.000000    0.000000           0.078000   21.000000    0.000000  \n",\
      "25%         0.000000   27.300000           0.243750   24.000000    0.000000  \n",\
      "50%        30.500000   32.000000           0.372500   29.000000    0.000000  \n",\
      "75%       127.250000   36.600000           0.626250   41.000000    1.000000  \n",\
      "max       846.000000   67.100000           2.420000   81.000000    1.000000  \n",\
      "{'options': {'params': {'mode': 'stage', 'algo': 'umap', 'n_components': '2'}, 'args': ['BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'response', 'serum_insulin', 'skin_thickness'], 'feature_variables': ['BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'response', 'serum_insulin', 'skin_thickness'], 'model_name': 'diabetes_umap', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'disabled': False, 'handle_new_cat': 'default', 'max_distinct_cat_values': '1000', 'max_distinct_cat_values_for_classifiers': '1000', 'max_distinct_cat_values_for_scoring': '1000', 'max_fit_time': '6000', 'max_inputs': '100000000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '150', 'max_score_time': '6000', 'streaming_apply': '0', 'use_sampling': '1'}, 'kfold_cv': None}, 'feature_variables': ['BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'response', 'serum_insulin', 'skin_thickness']}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"diabetes_umap\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 13,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 17,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 18,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    # model.fit()\n",\
    "    info = {\"message\": \"model trained\"}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 19,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{'message': 'model trained'}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 20,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    p = {\n",\
    "        \"n_neighbors\": 15,\n",\
    "        \"n_components\": 2\n",\
    "    }\n",\
    "    min_confidence = 0.0\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            for k in p.keys():\n",\
    "                if k in param['options']['params']:\n",\
    "                    p[k] = param['options']['params'][k]\n",\
    "    \n",\
    "    #reducer = umap.UMAP(random_state=42)\n",\
    "    reducer = umap.UMAP(\n",\
    "        random_state=42, \n",\
    "        n_neighbors=int(p['n_neighbors']),\n",\
    "        n_components=int(p['n_components'])\n",\
    "    )\n",\
    "\n",\
    "    embedding = reducer.fit_transform(X)\n",\
    "    result = pd.DataFrame(embedding).add_prefix(\"umap_\")\n",\
    "    return pd.concat([df,result], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 21,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "             0          1\n",\
      "0    15.441371   9.171429\n",\
      "1    12.556065   7.874014\n",\
      "2    15.691984  11.179146\n",\
      "3    -0.318175   4.113552\n",\
      "4    -3.303875   6.644968\n",\
      "..         ...        ...\n",\
      "763  -4.314896   7.316275\n",\
      "764  14.517242   7.850934\n",\
      "765  -1.093813   5.739352\n",\
      "766  13.944445  12.655777\n",\
      "767  12.764308   7.787235\n",\
      "\n",\
      "[768 rows x 2 columns]\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 5


[Named Entity Recognition and Extraction:fit]
[Named Entity Recognition and Extraction]
runtime = base
description = Named Entity Recognition using spaCy for NLP tasks
category = NLP
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit Notebook for Named Entity Recognition and Extraction with spaCy"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk Machine Learning Toolkit (MLTK) Container for TensorFlow. This script contains an example of how to run an entity extraction algorithm over text using the spacy library."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import spacy\n",\
    "\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"spacy version: \" + spacy.__version__)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import sys\n",\
    "!{sys.executable} -m spacy download en_core_web_sm"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| makeresults\n",\
    "| eval text = \"Boris Johnson has met Emmanuel Macron in Paris for Brexit talks, with the French president saying the UK's vote to quit the EU must be respected, but he added that the Ireland-Northern Ireland backstop plan was 'indispensable' to preserving political stability and the single market.;The backstop, opposed by Mr Johnson, aims to prevent a hard border on the island of Ireland after Brexit. Mr Johnson said that with 'energy and creativity we can find a way forward'.;On Wednesday German Chancellor Angela Merkel said the onus was on the UK to find a workable plan.;UK Prime Minister Mr Johnson insists the backstop must be ditched if a no-deal exit from the EU on 31 October is to be avoided.;He argues that it could leave the UK tied to the EU indefinitely, contrary to the result of the 2016 referendum, in which 52% of voters opted to leave.;But the EU has repeatedly said the withdrawal deal negotiated by former PM Theresa May, which includes the backstop, cannot be renegotiated.;However, it has previously said it would be willing to 'improve' the political declaration - the document that sets out the UK's future relationship with the EU.;Speaking after he greeted Mr Johnson at Paris's Elysee Palace, Mr Macron said he was 'very confident' that the UK and EU would be able to find a solution within 30 days - a timetable suggested by Mrs Merkel - 'if there is a good will on both sides'.;He said it would not be possible to find a new withdrawal agreement 'very different from the existing one' within that time, but added that an answer could be reached 'without reshuffling' the current deal.;Mr Macron also denied that he was the 'hard boy in the band', following suggestions that he would be tougher on the UK than his German counterpart.;Standing beside Mr Macron, Mr Johnson said he had been 'powerfully encouraged' by his conversations with Mrs Merkel in Berlin on Wednesday.;He emphasised his desire for a deal with the EU but added that it was 'vital for trust in politics' that the UK left the EU on 31 October.'He also said that 'under no circumstances' would the UK put checks or controls on the Ireland-UK border.;The two leaders ate lunch, drank coffee and walked through the Elysee gardens together during their talks, which lasted just under two hours. Mr Johnson then left to fly back to the UK.\"\n",\
    "| makemv text delim=\";\"\n",\
    "| mvexpand text\n",\
    "| fit MLTKContainer algo=spacy_ner epochs=100 text into app:spacy_entity_extraction_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"spacy_entity_extraction_model in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"spacy_entity_extraction_model\")\n",\
    "print(df)\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize the model\n",\
    "# params: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",\
    "    import en_core_web_sm\n",\
    "    model = en_core_web_sm.load()\n",\
    "    #model = spacy.load(\"en_core_web_sm\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note that for this algorithm the model is pre-trained (the en_core_web_sm library comes pre-packaged by spacy) and therefore this stage is a placeholder only"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# returns a fit info json object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    \n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].values.tolist()\n",\
    "    \n",\
    "    returns = list()\n",\
    "    \n",\
    "    for i in range(len(X)):\n",\
    "        doc = model(str(X[i]))\n",\
    "        \n",\
    "        \n",\
    "        entities = ''\n",\
    "    \n",\
    "        # Find named entities, phrases and concepts\n",\
    "        for entity in doc.ents:\n",\
    "            if entities == '':\n",\
    "                entities = entities + entity.text + ':' + entity.label_\n",\
    "            else:\n",\
    "                entities = entities + '|' + entity.text + ':' + entity.label_\n",\
    "        \n",\
    "        returns.append(entities)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = apply(model,df,param)\n",\
    "print(returns)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    # model will not be saved or reloaded as it is pre-built\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    # model will not be saved or reloaded as it is pre-built\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"spacy\": spacy.__version__} }\n",\
    "    if model is not None:\n",\
    "        # Save keras model summary to string:\n",\
    "        s = []\n",\
    "        returns[\"summary\"] = ''.join(s)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 2


[Graph Centrality Algorithms:fit]
[Graph Centrality Algorithms] 
runtime = base
description = Graph centrality algorithms using NetworkX
category = Graphs
source_code_version = 6
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Graph Algorithms with NetworkX"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains examples for graph algorithms available in NetworkX"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import networkx as nx\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"networkx version: \" + nx.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup bitcoin_transactions.csv<br>\n",\
    "| head 1000<br>\n",\
    "| rename user_id_from as src user_id_to as dest<br>\n",\
    "| fit MLTKContainer mode=stage algo=graph_algo compute=\"eigenvector_centrality,cluster_coefficient,betweenness_centrality\" from src dest into app:bitcoin_graph as graph"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"bitcoin_graph\")\n",\
    "print(df[0:1])\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = nx.Graph()\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    model.clear()\n",\
    "    src_dest_name = param['feature_variables']\n",\
    "    dfg = df[src_dest_name]\n",\
    "    for index, row in dfg.iterrows():\n",\
    "        model.add_edge(row[src_dest_name[0]], row[src_dest_name[1]]) #, value=row['value'])\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "g = fit(model,df,param)\n"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "param['options']['params']['compute'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    src_dest_name = param['feature_variables']\n",\
    "    algos = param['options']['params']['compute'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')\n",\
    "    outputcolumns = []\n",\
    "    results_df = df.copy()\n",\
    "    for algo in algos:\n",\
    "        if algo=='degree_centrality':\n",\
    "            cents = nx.algorithms.centrality.degree_centrality(model)\n",\
    "            outputcolumns.append(algo)\n",\
    "        elif algo=='betweenness_centrality':\n",\
    "            cents = nx.algorithms.centrality.betweenness_centrality(model)\n",\
    "            outputcolumns.append(algo)\n",\
    "        elif algo=='eigenvector_centrality':\n",\
    "            cents = nx.algorithms.centrality.eigenvector_centrality(model, max_iter=200)\n",\
    "            outputcolumns.append(algo)\n",\
    "        elif algo=='cluster_coefficient':\n",\
    "            cents = nx.algorithms.cluster.clustering(model)\n",\
    "            outputcolumns.append(algo)\n",\
    "        else:\n",\
    "            continue\n",\
    "        degs = pd.DataFrame(list(cents.items()), columns=[src_dest_name[0], algo])\n",\
    "        results_df = results_df.join(degs.set_index(src_dest_name[0]), on=src_dest_name[0])\n",\
    "    return pd.concat([df,results_df], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    # with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "    #    json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = init(None,None)\n",\
    "    # with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "    #    model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__, \"networkx\": nx.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python [conda env:root] *",\
   "language": "python",\
   "name": "conda-root-py"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\


[Correlation Matrix:summary]
[Correlation Matrix:fit]
[Correlation Matrix]
runtime = base
description = Correlation Matrix and Pair Plot
category = Basic
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Correlation Matrix and Pairplot Notebook"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import seaborn as sns\n",\
    "import matplotlib.pyplot as plt\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv</br>\n",\
    "| fit MLTKContainer algo=correlationmatrix plot=\"matrix,pairplot\" response from BMI age blood_pressure diabetes_pedigree glucose_concentration number_pregnant serum_insulin skin_thickness into app:diabetes_correlation"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"diabetes_correlation\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    # model.fit()\n",\
    "    info = {\"message\": \"no fit needed\"}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def plot_to_base64(plot):\n",\
    "    import base64\n",\
    "    import io \n",\
    "    pic_IObytes = io.BytesIO()\n",\
    "    if hasattr(plot,'fig'):\n",\
    "        plot.fig.savefig(pic_IObytes, format='png')\n",\
    "    elif hasattr(plot,'figure'):\n",\
    "        plot.figure.savefig(pic_IObytes, format='png')\n",\
    "    pic_IObytes.seek(0)\n",\
    "    pic_hash = base64.b64encode(pic_IObytes.read())\n",\
    "    return pic_hash\n",\
    "\n",\
    "\n",\
    "def plot_pairplot_as_base64(df,param):\n",\
    "    hue=None\n",\
    "    if 'options' in param:\n",\
    "        if 'target_variable' in param['options']:\n",\
    "            hue=str(param['options']['target_variable'][0])\n",\
    "    plot = sns.pairplot(df,hue=hue, palette=\"husl\")\n",\
    "    return str(plot_to_base64(plot))\n",\
    "\n",\
    "\n",\
    "def plot_correlationmatrix_as_base64(corr):\n",\
    "    # Set up the matplotlib figure\n",\
    "    f, ax = plt.subplots(figsize=(15, 15))\n",\
    "    # Generate a mask for the upper triangle\n",\
    "    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",\
    "    # Generate a custom diverging colormap\n",\
    "    cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",\
    "    # Draw the heatmap with the mask and correct aspect ratio\n",\
    "    #plot = sns.heatmap(corr, mask=mask, cmap=\"Spectral\", vmax=1.0, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",\
    "    plot = sns.heatmap(corr, cmap=\"Spectral\", vmax=1.0, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",\
    "    #plot.figure.savefig(\"plot.png\", format='png')\n",\
    "    return str(plot_to_base64(plot))\n",\
    "\n",\
    "def apply(model,df,param):\n",\
    "    # param['options']['model_name']    \n",\
    "    dfeatures = df[param['feature_variables']]\n",\
    "    result = dfeatures.corr() #.reset_index()\n",\
    "    if 'plot' in param['options']['params']:\n",\
    "        plots = param['options']['params']['plot'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')\n",\
    "        for plot in plots:\n",\
    "            if plot=='matrix':\n",\
    "                model[\"plot_matrix\"] = plot_correlationmatrix_as_base64(result)\n",\
    "            elif plot=='pairplot':\n",\
    "                model[\"plot_pairplot\"] = plot_pairplot_as_base64(df,param)\n",\
    "            else:\n",\
    "                continue\n",\
    "\n",\
    "    return result\n"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "apply(model,df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "        json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "model  = save(model,'correlationmatrix_diabetes_correlation')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "        model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "model = load('correlationmatrix_diabetes_correlation')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 4


[Distributed Random Forest Regressor:fit]
[Distributed Random Forest Regressor:fit_only]
[Distributed Random Forest Regressor:fit_and_score]
[Distributed Random Forest Regressor]
description = Distributed Random Forest using Spark RDD MLlib
category = Classifier
runtime = spark
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "import io\n",\
    "import json\n",\
    "import numpy as np\n",\
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",\
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",\
    "from pyspark.mllib.util import MLUtils\n",\
    "from pyspark.mllib.regression import LabeledPoint\n",\
    "from pyspark.mllib.linalg import Vectors\n",\
    "from pyspark.mllib.evaluation import RegressionMetrics\n",\
    "\n",\
    "def fit_only(sc, events):\n",\
    "    # Step 1: Prepare data as labeled points\n",\
    "    training_data = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    \n",\
    "    # Step 2: Train model using Random Forest Regressor\n",\
    "    model = RandomForest.trainRegressor(\n",\
    "        training_data,\n",\
    "        categoricalFeaturesInfo={},\n",\
    "        numTrees=10, \n",\
    "        featureSubsetStrategy=\"auto\",\n",\
    "        impurity=\"variance\",\n",\
    "        maxDepth=4,\n",\
    "        maxBins=32\n",\
    "    )\n",\
    "    return [{\"status\": \"success\"}]\n",\
    "\n",\
    "\n",\
    "\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    \n",\
    "    # Step 1: Prepare data as labeled points\n",\
    "    training_data = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    \n",\
    "    # Step 2: Train model using Random Forest Regressor\n",\
    "    model = RandomForest.trainRegressor(\n",\
    "        training_data,\n",\
    "        categoricalFeaturesInfo={},                        \n",\
    "        numTrees=10, \n",\
    "        featureSubsetStrategy=\"auto\",\n",\
    "        impurity=\"variance\",\n",\
    "        maxDepth=4,\n",\
    "        maxBins=32\n",\
    "    )\n",\
    "\n",\
    "    # Step 3: Compute predictions\n",\
    "    predictions = model.predict(events.map(lambda row: [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "\n",\
    "    # Step 4: Join predictions and return results\n",\
    "    results = events.zip(predictions) \n",\
    "    def format_results(t):\n",\
    "        e,p=t\n",\
    "        e[\"predicted_fare_amount\"]=round(p,1)\n",\
    "        return e\n",\
    "    return results.map(format_results)\n",\
    "\n",\
    "\n",\
    "\n",\
    "def fit_and_score(sc, events):\n",\
    "    rdd = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    model = RandomForest.trainRegressor(rdd, categoricalFeaturesInfo={},\n",\
    "                                numTrees=10, featureSubsetStrategy=\"auto\",\n",\
    "                                impurity=\"variance\", maxDepth=4, maxBins=32)    \n",\
    "\n",\
    "    predictions = model.predict(events.map(lambda row: [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "\n",\
    "    # Join prediction with input events\n",\
    "    results = events.zip(predictions) \n",\
    "    def format_results(t):\n",\
    "        e,p=t\n",\
    "        e[\"predicted_fare_amount\"]=p\n",\
    "        return e\n",\
    "    results = results.map(format_results)\n",\
    "    \n",\
    "\n",\
    "    # Calculate Scoring Metrics\n",\
    "    values_and_predictions = results.map(lambda p: (float(p[\"predicted_fare_amount\"]), float(p[\"fare_amount\"])))\n",\
    "\n",\
    "    metrics = RegressionMetrics(values_and_predictions)\n",\
    "    return [{\"MSE\": metrics.meanSquaredError, \n",\
    "             \"RMSE\": metrics.rootMeanSquaredError, \n",\
    "             \"R2\": metrics.r2, \n",\
    "             \"MAE\": metrics.meanAbsoluteError, \n",\
    "             \"Explained variance\": metrics.explainedVariance, \n",\
    "            }]"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}\

source_code_version = 35