[default] 
handler = 
connector = 
algorithm_handler = dltk.core.algorithm.Algorithm
deployment_handler = 
execution_handler = 
deployment_params = 
algorithm_params = 
source_code = 
deployment_code = 

[base]
connector = kubernetes
deployment_handler = dltk.runtime.base.BaseDeployment
execution_handler = dltk.runtime.base.BaseExecution
image = dltk4splunk/base-runtime:devel
cpu_count = 1
gpu_request = 
memory_mb = 2048
store_models_in_volume = true
algorithm_params = image
deployment_params = cpu_count, gpu_request, memory_mb, store_models_in_volume
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit 4.x for Splunk - Barebone Notebook"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly runs in Splunk Enterprise and interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_import"\
   },\
   "outputs": [],\
   "source": [\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | compute command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| makeresults count=10<br>\n",\
    "| streamstats c as i<br>\n",\
    "| eval s = i%3<br>\n",\
    "| eval feature_{s}=0<br>\n",\
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",\
    "| compute algorithm=\"Base Barebone\" mode=\"stage\" method=\"fit\" environment=\"DockerDev\" fields=\"feature_0,feature_1,feature_2,i,s\" model_name=\"barebone_model\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed and should only be used for staging data into the notebook environment to have it accessible in this notebook\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"barebone_model\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    model['hyperparameter'] = 42.0\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    # model.fit()\n",\
    "    info = {\"message\": \"model trained\"}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    y_hat = np.ones(df.shape[0]) * model['hyperparameter']\n",\
    "    result = pd.DataFrame(y_hat, columns=['meaning_of_life'])\n",\
    "    return pd.concat([df,result], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"algo_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "        json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "saved_model = save(model,'algo_barebone_model')\n",\
    "saved_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"algo_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "        model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "loaded_model = load('algo_barebone_model')\n",\
    "loaded_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "summary(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Finally you can reuse the model by calling your SPL with | compute ... method=\"apply\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| makeresults count=10<br>\n",\
    "| streamstats c as i<br>\n",\
    "| eval s = i%3<br>\n",\
    "| eval feature_{s}=0<br>\n",\
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",\
    "| compute algorithm=\"Base Barebone\" method=\"apply\" environment=\"DockerDev\" fields=\"feature_0,feature_1,feature_2,i,s\" model_name=\"barebone_model\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

[spark]
connector = kubernetes
deployment_handler = dltk.runtime.spark.SparkDeployment
execution_handler = dltk.runtime.spark.SparkExecution
editor_cpu_request = 100m
editor_cpu_limit = 1
editor_memory_mb = 500
editor_image = dltk4splunk/spark-runtime-editor:devel
driver_image = dltk4splunk/spark-runtime-driver:devel
executor_image = dltk4splunk/spark-runtime-executor:devel
driver_proxy_image = dltk4splunk/spark-runtime-driver-proxy:devel
inbound_relay_cpu_request = 100m
inbound_relay_cpu_limit = 1
inbound_relay_memory_mb = 500
outbound_relay_cpu_request = 100m
outbound_relay_cpu_limit = 1
outbound_relay_memory_mb = 500
inbound_relay_image = dltk4splunk/spark-runtime-inbound-relay:devel
outbound_relay_image = dltk4splunk/spark-runtime-outbound-relay:devel
executor_memory_mb = 1024
driver_memory_mb = 1024
spark_hdfs_url = 
relay_hdfs_url = 
input_hdfs_data_path = direct
spark_service_account = default
algorithm_params = editor_image, driver_image, executor_image, inbound_relay_image, outbound_relay_image, driver_proxy_image
deployment_params = editor_cpu_request, editor_cpu_limit, editor_memory_mb, executor_instance_count, checkpoint_url, checkpoint_s3_access_key, checkpoint_s3_secret_key, inbound_relay_cpu_request, inbound_relay_cpu_limit, inbound_relay_memory_mb, outbound_relay_cpu_request, outbound_relay_cpu_limit, outbound_relay_memory_mb, driver_memory_mb, executor_memory_mb, spark_hdfs_url, relay_hdfs_url, executor_cores
checkpoint_url = 
checkpoint_s3_access_key = 
checkpoint_s3_secret_key = 
executor_instance_count = 2
executor_cores = 2
receiver_count = 1
batch_interval = 1
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "import json\n",\
    "import numpy as np\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    return events"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}\
