{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Toolkit for Splunk - Spark MLLib FP Growth Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.18.4\n",
      "pandas version: 1.0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup supermarket.csv <br>\n",
    "| stats values(product_id) as basket by customer_id<br>\n",
    "| fit MLTKContainer mode=stage algo=spark_fp_growth min_support=0.1 min_confidence=0.0 min_items=2 limit=100 basket from customer_id into app:frequent_items <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'options': {'params': {'mode': 'stage', 'algo': 'spark_fp_growth', 'min_support': '0.6', 'min_confidence': '0.1', 'min_items': '2', 'max_items': '2', 'limit': '100'}, 'args': ['basket', 'customer_id'], 'target_variable': ['basket'], 'feature_variables': ['customer_id'], 'model_name': 'frequent_items', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'disabled': False, 'handle_new_cat': 'default', 'max_distinct_cat_values': '1000', 'max_distinct_cat_values_for_classifiers': '1000', 'max_distinct_cat_values_for_scoring': '1000', 'max_fit_time': '6000', 'max_inputs': '100000000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '150', 'max_score_time': '6000', 'streaming_apply': '0', 'use_sampling': '1'}, 'kfold_cv': None}, 'feature_variables': ['customer_id'], 'target_variables': ['basket']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>basket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u1</td>\n",
       "      <td>p112 p1174 p1249 p1251 p1252 p1572 p1595 p1602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u10</td>\n",
       "      <td>p1231 p1238 p1245 p1246 p126 p1270 p1415 p1418...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u100</td>\n",
       "      <td>p112 p1972 p218 p2756 p2854 p3049 p307 p3114 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u102</td>\n",
       "      <td>p1749 p1944 p2649 p2778 p3065 p3122 p3557 p505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u103</td>\n",
       "      <td>p1002 p1111 p1113 p1121 p1125 p1127 p1142 p115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>u91</td>\n",
       "      <td>p1 p1012 p1115 p1121 p1143 p1144 p1160 p1211 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>u92</td>\n",
       "      <td>p1114 p112 p1726 p1733 p2097 p2831 p2878 p2897...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>u93</td>\n",
       "      <td>p1036 p1116 p112 p1150 p1192 p1215 p150 p1595 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>u96</td>\n",
       "      <td>p1000 p1267 p147 p1674 p1705 p1706 p1729 p176 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>u98</td>\n",
       "      <td>p1159 p1930 p2746 p3003 p3306 p3632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id                                             basket\n",
       "0            u1  p112 p1174 p1249 p1251 p1252 p1572 p1595 p1602...\n",
       "1           u10  p1231 p1238 p1245 p1246 p126 p1270 p1415 p1418...\n",
       "2          u100  p112 p1972 p218 p2756 p2854 p3049 p307 p3114 p...\n",
       "3          u102  p1749 p1944 p2649 p2778 p3065 p3122 p3557 p505...\n",
       "4          u103  p1002 p1111 p1113 p1121 p1125 p1127 p1142 p115...\n",
       "..          ...                                                ...\n",
       "211         u91  p1 p1012 p1115 p1121 p1143 p1144 p1160 p1211 p...\n",
       "212         u92  p1114 p112 p1726 p1733 p2097 p2831 p2878 p2897...\n",
       "213         u93  p1036 p1116 p112 p1150 p1192 p1215 p150 p1595 ...\n",
       "214         u96  p1000 p1267 p147 p1674 p1705 p1706 p1729 p176 ...\n",
       "215         u98                p1159 p1930 p2746 p3003 p3306 p3632\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"frequent_items\")\n",
    "print(param)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent_items\n"
     ]
    }
   ],
   "source": [
    "print(param['options']['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id']\n",
      "basket\n"
     ]
    }
   ],
   "source": [
    "print(param['feature_variables'])\n",
    "print(param['target_variables'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    appName = \"fp_growth_spark_model\"\n",
    "    if 'options' in param:\n",
    "        if 'model_name' in param['options']: \n",
    "            appName = param['options']['model_name']\n",
    "    sparkConf = SparkConf().setAll([('spark.executor.memory', '1g'), ('spark.executor.cores', '1'), ('spark.cores.max', '4'), ('spark.driver.memory','4g'), ('spark.driver.maxResultSize','4g')])\n",
    "    spark = SparkSession.builder.appName(appName).config(conf=sparkConf).getOrCreate()\n",
    "    model['spark'] = spark\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spark': <pyspark.sql.session.SparkSession object at 0x7f6c94070450>}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5e5ceddf3bb3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>frequent_items</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c94070450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['spark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[('spark.app.name', 'frequent_items'), ('spark.driver.memory', '4g'), ('spark.app.id', 'local-1592389830151'), ('spark.executor.id', 'driver'), ('spark.driver.port', '46473'), ('spark.cores.max', '4'), ('spark.executor.memory', '1g'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.host', '5e5ceddf3bb3'), ('spark.driver.maxResultSize', '4g')]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(model['spark'].sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    spark = model['spark']\n",
    "    sc = spark.sparkContext\n",
    "    feature_variables = param['feature_variables']\n",
    "    target_variable = param['target_variables'][0]\n",
    "\n",
    "    min_support = 0.10\n",
    "    min_confidence = 0.10\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'min_support' in param['options']['params']:\n",
    "                min_support = float(param['options']['params']['min_support'])\n",
    "            if 'min_confidence' in param['options']['params']:\n",
    "                min_confidence = float(param['options']['params']['min_confidence'])\n",
    "\n",
    "    df['_items'] = df[target_variable].map(lambda l: l.split(' '))\n",
    "    sdf = spark.createDataFrame(df)\n",
    "\n",
    "    model['fpgrowth'] = FPGrowth(itemsCol='_items', minSupport=min_support, minConfidence=min_confidence)\n",
    "    model['model'] = model['fpgrowth'].fit(sdf)\n",
    "\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'model trained'}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['model'].freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['model'].associationRules.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# apply your model\n",
    "# returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    limit = 10\n",
    "    min_items = 2\n",
    "    max_items = 2\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'limit' in param['options']['params']:\n",
    "                limit = int(param['options']['params']['limit'])\n",
    "            if 'min_items' in param['options']['params']:\n",
    "                min_items = int(param['options']['params']['min_items'])\n",
    "            if 'max_items' in param['options']['params']:\n",
    "                max_items = int(param['options']['params']['max_items'])\n",
    "\n",
    "    spark = model['spark']\n",
    "    freqItems = model['model'].freqItemsets\n",
    "    freqItems.createOrReplaceTempView(\"frequentItems\")\n",
    "    results = spark.sql(\"select items, freq from frequentItems where size(items) between \"+str(min_items)+\" and \"+str(max_items)+\" order by freq desc limit \"+str(limit))\n",
    "    result = results.toPandas()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             items  freq\n",
      "0    [p2098, p112]   187\n",
      "1    [p2096, p112]   183\n",
      "2    [p1669, p112]   182\n",
      "3    [p2358, p112]   179\n",
      "4    [p3041, p112]   177\n",
      "..             ...   ...\n",
      "95  [p2785, p3041]   153\n",
      "96   [p710, p2785]   153\n",
      "97  [p3122, p2358]   153\n",
      "98   [p710, p1930]   153\n",
      "99  [p2033, p1234]   153\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    #import shutil\n",
    "    #from pathlib import Path\n",
    "    #if Path(MODEL_DIRECTORY + name).is_dir():\n",
    "    #    shutil.rmtree(MODEL_DIRECTORY + name)\n",
    "    #model['model'].save(model['spark'].sparkContext, MODEL_DIRECTORY + name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model,'spark_fp_growth_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = init({},{})\n",
    "    #model['model'] = GradientBoostedTreesModel.load(model['spark'].sparkContext, MODEL_DIRECTORY + name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('spark_fp_growth_test')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"spark\": \"no model\"}\n",
    "    if model:\n",
    "        returns = {\"spark_info\": str(model['spark'].sparkContext.getConf().getAll()) }\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
