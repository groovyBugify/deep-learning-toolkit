{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Splunk Machine Learning Toolkit Container for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk Machine Learning Toolkit (MLTK) Container for TensorFlow. As an example we use a custom classifier built on keras and tensorflow.\n",
    "Note: All code cells below have metadata attached to be reusable in the underlying MLTK Container for TensorFlow extension and are marked as non deletable and follow the naming convention mltkc_*. Feel free to add your own cells for expermentation but make sure every production ready code should live in the existing staging cells. By default every time you save this notebook the cells are exported into a python module which is then used for running your custom model invoked by Splunk MLTK commands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# mltkc_import\n",
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.16.2\n",
      "pandas version: 0.24.2\n",
      "TensorFlow version: 2.0.0-alpha0\n",
      "Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"Keras version: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup server_power.csv<br>| fit MLTKContainer mode=stage algo=linear_regressor epochs=10 batch_size=32 ac_power from total* into app:server_power_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"my_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# mltkc_stage\n",
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ac_power  total-unhalted_core_cycles  total-instructions_retired  \\\n",
      "0     220.0                   4708152.0                   3924639.0   \n",
      "\n",
      "   total-last_level_cache_references  total-memory_bus_transactions  \\\n",
      "0                            75140.0                         5130.0   \n",
      "\n",
      "   total-cpu-utilization  total-disk-accesses  total-disk-blocks  \\\n",
      "0                   0.99                  0.0                0.0   \n",
      "\n",
      "   total-disk-utilization  \n",
      "0                     0.0  \n",
      "(31271, 9)\n",
      "{'options': {'args': ['ac_power', 'total*'], 'model_name': 'server_power_regression', 'algo_name': 'MLTKContainer', 'params': {'batch_size': '32', 'mode': 'stage', 'epochs': '10', 'algo': 'linear_regressor'}, 'feature_variables': ['total*'], 'mlspl_limits': {'handle_new_cat': 'default', 'use_sampling': 'true', 'streaming_apply': 'false', 'max_model_size_mb': '1500', 'max_distinct_cat_values_for_scoring': '1000', 'max_score_time': '60000', 'max_memory_usage_mb': '12000', 'max_inputs': '1000000000', 'max_fit_time': '60000', 'max_distinct_cat_values_for_classifiers': '1000', 'max_distinct_cat_values': '1000'}, 'kfold_cv': None, 'target_variable': ['ac_power'], 'search_info': {'session_key': 'FBftofwCsETTYoFFS2KbrAdDYraO7FyZYsSOvrqkYKczTILvC1Owo_EePzZxLmV31XL1jpFqEOpA2ZkErnTymS5XTb62^18Atf2gF9aO40K1b1uI7^XhSc56CXsdfN', 'app': 'mltk-container', 'splunkd_uri': 'https://127.0.0.1:8089', 'username': 'admin', 'sid': '1569836257.17320'}}, 'target_variables': ['ac_power'], 'feature_variables': ['total-unhalted_core_cycles', 'total-instructions_retired', 'total-last_level_cache_references', 'total-memory_bus_transactions', 'total-cpu-utilization', 'total-disk-accesses', 'total-disk-blocks', 'total-disk-utilization']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"server_power_regression\")\n",
    "print(df[0:1])\n",
    "print(df.shape)\n",
    "print(str(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# mltkc_init\n",
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    X = df[param['feature_variables']]\n",
    "    print(\"FIT build model with input shape \" + str(X.shape))\n",
    "    learning_rate = 0.1\n",
    "    model_name = \"default_linear_regressor\"\n",
    "    if 'options' in param:\n",
    "        if 'model_name' in param['options']:\n",
    "            model_name = param['options']['model_name']\n",
    "        if 'params' in param['options']:\n",
    "            if 'learning_rate' in param['options']['params']:\n",
    "                learning_rate = int(param['options']['params']['learning_rate'])\n",
    "\n",
    "    feature_columns = []\n",
    "    for feature_name in param['feature_variables']:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "    \n",
    "    model = tf.estimator.LinearRegressor(\n",
    "        feature_columns=feature_columns,\n",
    "        model_dir=MODEL_DIRECTORY + model_name + \"/\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 15:00:21.337473 140563149854464 estimator.py:1799] Using temporary folder as model directory: /tmp/tmp3cvxrv6b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT build model with input shape (31271, 8)\n",
      "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 object at 0x7fd70b118f60>\n"
     ]
    }
   ],
   "source": [
    "# test mltkc_stage_create_model\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_fit\n",
    "# returns a fit info json object\n",
    "def make_input_fn(df, param, n_epochs=None, batch_size=None, shuffle=True):\n",
    "    def input_fn():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df[param['feature_variables']].to_dict(orient='list'), df[param['target_variables']].values))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(df))\n",
    "        return dataset.repeat(n_epochs).batch(batch_size)\n",
    "    return input_fn\n",
    "\n",
    "def fit(model,df,param):\n",
    "    returns = {}\n",
    "    X = df[param['feature_variables']]\n",
    "    model_epochs = 100\n",
    "    model_batch_size = 32\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'epochs' in param['options']['params']:\n",
    "                model_epochs = int(param['options']['params']['epochs'])\n",
    "            if 'batch_size' in param['options']['params']:\n",
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",
    "    # connect model training to tensorboard\n",
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # run the training\n",
    "    input_fn_train = make_input_fn(df,param,model_epochs,model_batch_size)\n",
    "    model.train(input_fn=input_fn_train, max_steps=model_epochs)\n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model_epochs\n",
    "    returns['model_batch_size'] = model_batch_size\n",
    "    returns['model_loss_acc'] = model.evaluate(input_fn=input_fn_train)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 15:00:24.546078 140563149854464 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0930 15:00:25.221930 140563149854464 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2758: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0930 15:00:25.671239 140563149854464 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0930 15:00:29.653259 140563149854464 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction/mean': 76.48488, 'loss': 33587.402, 'average_loss': 33587.27, 'label/mean': 239.80342, 'global_step': 10}\n"
     ]
    }
   ],
   "source": [
    "returns = fit(model,df,param)\n",
    "print(returns['model_loss_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_apply\n",
    "def apply(model,df,param):\n",
    "    X = df[param['feature_variables']]\n",
    "    model_epochs = 1\n",
    "    model_batch_size = 32\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'batch_size' in param['options']['params']:\n",
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",
    "    output_fn_train = make_input_fn(df,param,model_epochs,model_batch_size)\n",
    "    y_hat = pd.DataFrame([p['predictions'] for p in list(model.predict(output_fn_train))])\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "0       69.204559\n",
      "1       33.916595\n",
      "2       39.320969\n",
      "3      269.523132\n",
      "4      197.207748\n",
      "5       25.040310\n",
      "6       28.348602\n",
      "7      326.383362\n",
      "8       10.051042\n",
      "9      138.244644\n",
      "10     302.008362\n",
      "11     217.782639\n",
      "12      37.107815\n",
      "13      51.944801\n",
      "14     346.025787\n",
      "15     115.518867\n",
      "16      54.119675\n",
      "17     338.388031\n",
      "18      15.495632\n",
      "19     261.154114\n",
      "20     140.740417\n",
      "21      15.510792\n",
      "22      17.191122\n",
      "23      24.691265\n",
      "24      20.562876\n",
      "25       0.680564\n",
      "26      34.411102\n",
      "27       5.885104\n",
      "28       7.791601\n",
      "29      45.855240\n",
      "...           ...\n",
      "31241   44.834221\n",
      "31242  114.942581\n",
      "31243   33.703789\n",
      "31244    8.083603\n",
      "31245   45.863754\n",
      "31246   69.659668\n",
      "31247   45.358166\n",
      "31248  141.835159\n",
      "31249  189.336761\n",
      "31250   38.920429\n",
      "31251   33.252640\n",
      "31252   30.828377\n",
      "31253  483.440491\n",
      "31254  330.580627\n",
      "31255   23.038954\n",
      "31256   25.550045\n",
      "31257   39.766743\n",
      "31258    8.223471\n",
      "31259    7.910411\n",
      "31260    0.752985\n",
      "31261  303.735443\n",
      "31262    0.148260\n",
      "31263   42.335648\n",
      "31264   33.515488\n",
      "31265    5.339501\n",
      "31266   69.827896\n",
      "31267   35.140526\n",
      "31268  208.792084\n",
      "31269   32.976120\n",
      "31270   55.587082\n",
      "\n",
      "[31271 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# test mltkc_stage_create_model_apply\n",
    "y_hat = apply(model,df,param)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    # model.save(MODEL_DIRECTORY + name + \".h5\")\n",
    "    # serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(tf.feature_column.make_parse_example_spec([input_column]))\n",
    "    # export_path = model.export_saved_model(MODEL_DIRECTORY + name +\"/\", serving_input_fn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    # model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",
    "    if model is not None:\n",
    "        returns[\"summary\"] = \"linear regressor\"\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
